{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.read_csv(\"../input/store.csv\")\n",
    "logger.debug(\"Read store data: {0} rows, {1} columns\".format(*store.shape))\n",
    "\n",
    "if REDUCE_DATA:\n",
    "    logger.debug(\"DROPPING DATA FOR DEBUGGING\")\n",
    "    store = store.iloc[:50, :]\n",
    "    logger.debug(\"Store data reduced to {0} rows\".format(store.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(row):\n",
    "    \"\"\"\n",
    "    Combines the year and month values to return a date\n",
    "    :param row: DataFrame row\n",
    "    :return: Timestamp for the date\n",
    "    \"\"\"\n",
    "    year, month = row[['CompetitionOpenSinceYear', 'CompetitionOpenSinceMonth']]\n",
    "    if not pd.isnull(year):\n",
    "        return pd.Timestamp(int(year), int(month), 1)\n",
    "\n",
    "\n",
    "store['CompetitionOpenDate'] = store.apply(get_date, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"../input/train.csv\", low_memory=False)\n",
    "logger.debug(\"Read train data: {0} rows, {1} columns\".format(*train_csv.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv['Open'] = train_csv['Sales'].apply(lambda s: 1 if s > 0 else 0)\n",
    "assert check_nulls(train_csv, 'Open')\n",
    "logger.debug(\"Patch train_csv Open\")\n",
    "\n",
    "train_csv['Date'] = pd.to_datetime(train_csv['Date'])\n",
    "assert check_nulls(train_csv, 'Date')\n",
    "logger.debug(\"Convert train_csv date to datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate DatetimeIndex for the range of dates in the training set\n",
    "train_range = pd.date_range(train_csv['Date'].min(), train_csv['Date'].max(), name='Date')\n",
    "logger.debug(\"Training range is {0} - {1}\".format(train_range.min(), train_range.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in gaps in dates for each store\n",
    "\n",
    "def fill_gaps_by_store(g):\n",
    "    s = g['Store'].iloc[0]\n",
    "    logger.debug(\"Store {s} initial shape {shape}\"\n",
    "                 .format(s=s, shape=g.shape))\n",
    "    filled = (g.set_index('Date')\n",
    "              .reindex(train_range)\n",
    "              .fillna(value={'Store': s, 'Sales': 0, 'Customers': 0, 'Open': 0,\n",
    "                             'Promo': 0, 'StateHoliday': '0',\n",
    "                             'SchoolHoliday': 0}))\n",
    "    filled['DayOfWeek'] = filled.index.weekday + 1\n",
    "    logger.debug(\"Store {s} shape after filling {shape}\"\n",
    "                 .format(s=s, shape=filled.shape))\n",
    "    return filled.reset_index()\n",
    "\n",
    "\n",
    "#  DayOfWeek: Monday is 0, Sunday is 6\n",
    "logger.debug(\"Expand index of each store to cover full date range\")\n",
    "train_full = train_csv.groupby('Store').apply(fill_gaps_by_store).reset_index(drop=True)\n",
    "assert check_nulls(train_full)\n",
    "logger.debug(\"Expanded train data from shape {0} to {1}\".format(train_csv.shape, train_full.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv(\"../input/test.csv\", low_memory=False)\n",
    "logger.debug(\"Read test data: {0} rows, {1} columns\".format(*test_csv.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv['Open'].fillna(value=1, inplace=True)\n",
    "assert check_nulls(test_csv, 'Open')\n",
    "logger.debug(\"Fill nas test_csv Open\")\n",
    "test_csv['Date'] = pd.to_datetime(test_csv['Date'])\n",
    "assert check_nulls(test_csv, 'Date')\n",
    "logger.debug(\"Convert test_csv date to datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate DatetimeIndex for the range of dates in the testing set\n",
    "# and the full range over both training and test sets.\n",
    "\n",
    "test_range = pd.date_range(test_csv['Date'].min(), test_csv['Date'].max(), name='Date')\n",
    "logger.debug(\"Test data range is {0} - {1}\".format(test_range.min(), test_range.max()))\n",
    "\n",
    "full_range = pd.date_range(min(train_csv['Date'].min(), test_csv['Date'].min()),\n",
    "                           max(train_csv['Date'].max(), test_csv['Date'].max()), name='Date')\n",
    "logger.debug(\"Full data range is {0} - {1}\".format(full_range.min(), full_range.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Fourier terms\n",
    "\n",
    "def fourier(ts_length, period, terms):\n",
    "    \"\"\"\n",
    "    Generate periodic terms with the given period\n",
    "    :param ts_length: The length of the generated terms (corresponding to the\n",
    "    time series length)\n",
    "    :param period: The period\n",
    "    :param terms: The number of series to generate.\n",
    "    :return: DataFrame of shape ts_length, 2*terms. Columns are named\n",
    "    s1, c1, s2, c2, ..., s<terms>, c<terms>\n",
    "    \"\"\"\n",
    "    A = pd.DataFrame()\n",
    "    fns = OrderedDict([('s', np.sin), ('c', np.cos)])\n",
    "    terms_range = range(1, terms + 1)\n",
    "    for term, (fn_name, fn) in product(terms_range, fns.items()):\n",
    "        A[fn_name + str(term)] = fn(2 * np.pi * term * np.arange(1, ts_length + 1) / period)\n",
    "    logger.debug(\"Created fourier terms of shape {0}\".format(A.shape))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_terms = 5\n",
    "\n",
    "fourier_features = fourier(len(full_range), period=365, terms=fourier_terms)\n",
    "fourier_names = ['Fourier' + str(x) for x in range(1, 2 * fourier_terms + 1)]\n",
    "fourier_features.columns = fourier_names\n",
    "fourier_features['Date'] = full_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_linear_features = [\"Promo\", \"Promo2Active\", \"SchoolHoliday\",\n",
    "                        \"DayOfWeek1\", \"DayOfWeek2\", \"DayOfWeek2\", \"DayOfWeek3\",\n",
    "                        \"DayOfWeek4\", \"DayOfWeek5\", \"DayOfWeek6\",\n",
    "                        \"StateHolidayA\", \"StateHolidayB\", \"StateHolidayC\",\n",
    "                        \"CompetitionOpen\", \"Open\"]\n",
    "\n",
    "trend_features = [\"DateTrend\", \"DateTrendLog\"]\n",
    "\n",
    "decay_features = [\"PromoDecay\", \"Promo2Decay\",\n",
    "                  \"StateHolidayCLastDecay\", \"StateHolidayCNextDecay\",\n",
    "                  \"StateHolidayBNextDecay\"]\n",
    "\n",
    "log_decay_features = list(map(\"{0}Log\".format, decay_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stairs_steps = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 21, 28)\n",
    "\n",
    "stairs_features = chain(\n",
    "    [\"Opened\", \"PromoStarted\", \"Promo2Started\", \"TomorrowClosed\",\n",
    "     \"WasClosedOnSunday\"],\n",
    "    map(\"PromoStartedLastDate{0}after\".format, (2, 3, 4)),\n",
    "    starmap(\"{1}{0}before\".format,\n",
    "            product(stairs_steps, (\"StateHolidayCNextDate\", \"StateHolidayBNextDate\",\n",
    "                                   \"StateHolidayANextDate\", \"LongClosedNextDate\"))),\n",
    "    starmap(\"{1}{0}after\".format,\n",
    "            product(stairs_steps, (\"StateHolidayCLastDate\", \"StateHolidayBLastDate\",\n",
    "                                   \"Promo2StartedDate\", \"LongOpenLastDate\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_day_features = map(\"MDay{0}\".format, range(1, 32))\n",
    "\n",
    "month_features = map(\"Month{0}\".format, range(1, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_features = chain(base_linear_features, trend_features, decay_features,\n",
    "                        log_decay_features, stairs_features, fourier_names,\n",
    "                        month_day_features, month_features)\n",
    "\n",
    "glm_features = (\"Promo\", \"SchoolHoliday\",\n",
    "                \"DayOfWeek1\", \"DayOfWeek2\", \"DayOfWeek2\", \"DayOfWeek3\",\n",
    "                \"DayOfWeek4\", \"DayOfWeek5\", \"DayOfWeek6\",\n",
    "                \"StateHolidayA\", \"StateHolidayB\", \"StateHolidayC\",\n",
    "                \"CompetitionOpen\", \"PromoDecay\", \"Promo2Decay\",\n",
    "                \"DateTrend\", \"DateTrendLog\",\n",
    "                \"StateHolidayCLastDecay\", \"StateHolidayCNextDecay\",\n",
    "                \"Fourier1\", \"Fourier2\", \"Fourier3\", \"Fourier4\")\n",
    "\n",
    "log_lm_features = (\"Promo\", \"Promo2Active\", \"SchoolHoliday\",\n",
    "                   \"DayOfWeek1\", \"DayOfWeek2\", \"DayOfWeek2\", \"DayOfWeek3\",\n",
    "                   \"DayOfWeek4\", \"DayOfWeek5\", \"DayOfWeek6\",\n",
    "                   \"StateHolidayA\", \"StateHolidayB\", \"StateHolidayC\",\n",
    "                   \"CompetitionOpen\", \"PromoDecay\", \"Promo2Decay\",\n",
    "                   \"DateTrend\", \"DateTrendLog\",\n",
    "                   \"StateHolidayCLastDecay\", \"StateHolidayCNextDecay\",\n",
    "                   \"Fourier1\", \"Fourier2\", \"Fourier3\", \"Fourier4\")\n",
    "\n",
    "categorical_numeric_features = chain(\n",
    "    (\"Store\", \"Promo\", \"Promo2Active\", \"SchoolHoliday\", \"DayOfWeek\",\n",
    "     \"StateHolidayN\", \"CompetitionOpen\", \"StoreTypeN\", \"AssortmentN\",\n",
    "     \"DateTrend\", \"MDay\", \"Month\", \"Year\"), decay_features, stairs_features,\n",
    "    fourier_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_date = pd.to_datetime(\"2000-01-01\")\n",
    "future_date = pd.to_datetime(\"2099-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data\n",
    "tftc = pd.concat([train_full, test_csv])\n",
    "logger.debug(\"Combined train and test data. Train data shape {0}. Test data shape {1}. Combined data shape {2}\".format(\n",
    "    train_full.shape, test_csv.shape, tftc.shape\n",
    "))\n",
    "\n",
    "joined = pd.merge(tftc, store, on='Store', how='inner')\n",
    "logger.debug(\"Merged with store data, shape {0}\".format(joined.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = pd.isnull(joined).any()\n",
    "print(__.sum())\n",
    "__[__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add binary feature for each day of week.\n",
    "# ToDo: add term for Sunday.\n",
    "for i in range(1, 7):\n",
    "    joined['DayOfWeek{i}'.format(i=i)] = (joined['DayOfWeek']\n",
    "                                          .apply(lambda s: 1 if s == i else 0))\n",
    "    assert check_nulls(joined, 'DayOfWeek{i}'.format(i=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = pd.isnull(joined).any()\n",
    "print(__.sum())\n",
    "__[__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add binary features for StateHoliday categories, and one numerical feature\n",
    "for i in 'ABC':\n",
    "    joined['StateHoliday{i}'.format(i=i)] = (joined['StateHoliday']\n",
    "                                             .apply(lambda s: 1 if s == i.lower() else 0))\n",
    "    assert check_nulls(joined, 'StateHoliday{i}'.format(i=i))\n",
    "\n",
    "letter_to_number = {'0': 1, 'a': 2, 'b': 3, 'c': 4}\n",
    "\n",
    "joined['StateHolidayN'] = joined['StateHoliday'].apply(lambda x: letter_to_number[x])\n",
    "assert check_nulls(joined, 'StateHolidayN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = pd.isnull(joined).any()\n",
    "print(__.sum())\n",
    "__[__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined['CompetitionOpen'] = (joined['Date']\n",
    "                             >= joined['CompetitionOpenDate']).astype(int)\n",
    "assert check_nulls(joined, 'CompetitionOpen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_to_number = {'a': 1, 'b': 2, 'c': 3, 'd': 4}\n",
    "\n",
    "joined['StoreTypeN'] = joined['StoreType'].apply(lambda x: letter_to_number[x])\n",
    "joined['AssortmentN'] = joined['Assortment'].apply(lambda x: letter_to_number[x])\n",
    "assert check_nulls(joined, 'StoreTypeN')\n",
    "assert check_nulls(joined, 'AssortmentN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent date offsets\n",
    "joined['DateTrend'] = ((joined['Date'] - joined['Date'].min())\n",
    "                       .apply(lambda s: s.days + 1))\n",
    "assert check_nulls(joined, 'DateTrend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = pd.isnull(joined).any()\n",
    "print(__.sum())\n",
    "__[__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary feature to indicate if Promo2 is active\n",
    "# ToDo: Check interpretation\n",
    "month_to_nums = {'Jan,Apr,Jul,Oct': [1, 4, 7, 10],\n",
    "                 'Feb,May,Aug,Nov': [2, 5, 8, 11],\n",
    "                 'Mar,Jun,Sept,Dec': [3, 6, 9, 12]}\n",
    "\n",
    "\n",
    "def is_promo2_active(row):\n",
    "    if row['Promo2']:\n",
    "        start_year = row['Promo2SinceYear']\n",
    "        start_week = row['Promo2SinceWeek']\n",
    "        interval = row['PromoInterval']\n",
    "        date = row['Date']\n",
    "        if ((date.year > start_year)\n",
    "            or ((date.year == start_year) and (date.week >= start_week))):\n",
    "            if date.month in month_to_nums[interval]:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "joined['Promo2Active'] = joined[['Promo2', 'Promo2SinceYear',\n",
    "                                 'Promo2SinceWeek', 'PromoInterval',\n",
    "                                 'Date']].apply(is_promo2_active, axis=1)\n",
    "assert check_nulls(joined, 'Promo2Active')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical feature for PromoInterval\n",
    "month_to_nums = {'Jan,Apr,Jul,Oct': 3,\n",
    "                 'Feb,May,Aug,Nov': 2,\n",
    "                 'Mar,Jun,Sept,Dec': 4}\n",
    "\n",
    "joined['PromoIntervalN'] = (joined['PromoInterval']\n",
    "                            .apply(lambda s: 1 if pd.isnull(s) else month_to_nums[s]))\n",
    "assert check_nulls(joined, 'PromoIntervalN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day, month, year\n",
    "joined['MDay'] = joined['Date'].apply(lambda s: s.day)\n",
    "joined['Month'] = joined['Date'].apply(lambda s: s.month)\n",
    "joined['Year'] = joined['Date'].apply(lambda s: s.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary feature for day\n",
    "for i in range(1, 32):\n",
    "    joined['MDay{i}'.format(i=i)] = (joined['MDay']\n",
    "                                     .apply(lambda s: 1 if s == i else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary feature for month\n",
    "for i in range(1, 13):\n",
    "    joined['Month{i}'.format(i=i)] = (joined['Month']\n",
    "                                      .apply(lambda s: 1 if s == i else 0))\n",
    "\n",
    "logger.debug(\"Generated direct features, new shape {0}\".format(joined.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = pd.isnull(joined).any()\n",
    "print(__.sum())\n",
    "__[__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations grouped by Store\n",
    "\n",
    "def apply_grouped_by_store(g):\n",
    "    s = g['Store'].iloc[0]\n",
    "    logger.debug(\"Store {s} initial shape {shape}\".format(s=s, shape=g.shape))\n",
    "    g = date_features(g)\n",
    "    logger.debug(\"Store {s} after date features shape {shape}\".format(s=s, shape=g.shape))\n",
    "    g = merge_with_fourier_features(g)\n",
    "    logger.debug(\"Store {s} final shape {shape}\".format(s=s, shape=g.shape))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge fourier features\n",
    "def merge_with_fourier_features(g):\n",
    "    return pd.merge(g, fourier_features, on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_features(g):\n",
    "    g['PromoStarted'] = g['Promo'].diff().apply(lambda s: 1 if s > 0 else 0)\n",
    "    g['Promo2Started'] = g['Promo2Active'].diff().apply(lambda s: 1 if s > 0 else 0)\n",
    "    g['Opened'] = g['Open'].diff().apply(lambda s: 1 if s > 0 else 0)\n",
    "    g['Closed'] = g['Open'].diff().apply(lambda s: 1 if s < 0 else 0)\n",
    "    g['TomorrowClosed'] = g['Closed'].shift(-1).fillna(0)\n",
    "\n",
    "    # These are incomplete, NA values filled later.\n",
    "    g['PromoStartedLastDate'] = g.loc[g['PromoStarted'] == 1, 'Date']\n",
    "    g['Promo2StartedDate'] = g.loc[g['Promo2Started'] == 1, 'Date']\n",
    "    g['StateHolidayCLastDate'] = g.loc[g['StateHoliday'] == \"c\", 'Date']\n",
    "    g['StateHolidayCNextDate'] = g['StateHolidayCLastDate']\n",
    "    g['StateHolidayBLastDate'] = g.loc[g['StateHoliday'] == \"b\", 'Date']\n",
    "    g['StateHolidayBNextDate'] = g['StateHolidayBLastDate']\n",
    "    g['StateHolidayANextDate'] = g.loc[g['StateHoliday'] == \"a\", 'Date']\n",
    "    g['ClosedLastDate'] = g.loc[g['Closed'] == 1, 'Date']\n",
    "    g['ClosedNextDate'] = g['ClosedLastDate']\n",
    "    g['OpenedLastDate'] = g.loc[g['Opened'] == 1, 'Date']\n",
    "    g['OpenedNextDate'] = g['OpenedLastDate']\n",
    "    g['LastClosedSundayDate'] = g.loc[(g['Open'] == 0) & (g['DayOfWeek'] == 7), 'Date']\n",
    "\n",
    "    # Last dates filled with pad\n",
    "    features = ['PromoStartedLastDate',\n",
    "                'Promo2StartedDate',\n",
    "                'StateHolidayCLastDate',\n",
    "                'StateHolidayBLastDate',\n",
    "                'ClosedLastDate',\n",
    "                'OpenedLastDate',\n",
    "                'LastClosedSundayDate'\n",
    "                ]\n",
    "    g[features] = (g[features].fillna(method='pad')\n",
    "                   .fillna(value=pd.Timestamp('1970-01-01 00:00:00')))\n",
    "    assert check_nulls(g, features)\n",
    "\n",
    "    # ToDo: check interpretation\n",
    "    g['IsClosedForDays'] = (g['Date'] - g['ClosedLastDate']).dt.days\n",
    "\n",
    "    g['LongOpenLastDate'] = (g.loc[(g['Opened'] == 1)\n",
    "                                   & (g['IsClosedForDays'] > 5)\n",
    "                                   & (g['IsClosedForDays'] < 180),\n",
    "                                   'Date'])\n",
    "\n",
    "    # Last dates filled with pad\n",
    "    features = ['LongOpenLastDate',\n",
    "                ]\n",
    "    g[features] = (g[features].fillna(method='pad')\n",
    "                   .fillna(value=pd.Timestamp('1970-01-01 00:00:00')))\n",
    "    assert check_nulls(g, features)\n",
    "\n",
    "    #\n",
    "    g.loc[(g['Open'] == 0) & (g['DayOfWeek'] == 7), 'WasClosedOnSunday'] = 1\n",
    "    g['WasClosedOnSunday'].fillna(method='pad', limit=6)\n",
    "\n",
    "    # Next dates filled with backfill\n",
    "    features = ['StateHolidayCNextDate',\n",
    "                'OpenedNextDate',\n",
    "                'ClosedNextDate',\n",
    "                'StateHolidayBNextDate',\n",
    "                'StateHolidayANextDate'\n",
    "                ]\n",
    "    g[features] = (g[features].fillna(method='backfill')\n",
    "                   .fillna(value=pd.Timestamp('2020-01-01 00:00:00')))\n",
    "    assert check_nulls(g, features)\n",
    "\n",
    "    # ToDo: check interpretation\n",
    "    g['WillBeClosedForDays'] = (g['OpenedNextDate'] - g['Date']).dt.days\n",
    "\n",
    "    g['LongClosedNextDate'] = (g.loc[(g['Closed'] == 1)\n",
    "                                     & (g['WillBeClosedForDays'] > 5)\n",
    "                                     & (g['WillBeClosedForDays'] < 180),\n",
    "                                     'Date'])\n",
    "\n",
    "    # Next dates filled with backfill\n",
    "    features = ['LongClosedNextDate',\n",
    "                ]\n",
    "    g[features] = (g[features].fillna(method='backfill')\n",
    "                   .fillna(value=pd.Timestamp('2020-01-01 00:00:00')))\n",
    "    assert check_nulls(g, features)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfj = date_features(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfj[['PromoStartedLastDate',\n",
    "                'Promo2StartedDate',\n",
    "                'StateHolidayCLastDate',\n",
    "                'StateHolidayBLastDate',\n",
    "                'ClosedLastDate',\n",
    "                'OpenedLastDate',\n",
    "                'LastClosedSundayDate'\n",
    "                ]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = joined.groupby('Store').apply(apply_grouped_by_store)\n",
    "logger.debug('Expanded with date and fourier features shape {0}'.format(joined.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined['Store'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(joined[['PromoStartedLastDate',\n",
    "                'Promo2StartedDate',\n",
    "                'StateHolidayCLastDate',\n",
    "                'StateHolidayBLastDate',\n",
    "                'ClosedLastDate',\n",
    "                'OpenedLastDate',\n",
    "                'LastClosedSundayDate'\n",
    "                ]]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = pd.isnull(joined).any()\n",
    "print(__.sum())\n",
    "__[__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decay_features(g, promo_after, promo2_after, holiday_b_before,\n",
    "                        holiday_c_before, holiday_c_after):\n",
    "    g['PromoDecay'] = g['Promo'] * np.maximum(\n",
    "        promo_after - (g['Date'] - g['PromoStartedLastDate']).dt.days\n",
    "        , 0)\n",
    "    g['StateHolidayCLastDecay'] = (1 - g['StateHolidayC']) * np.maximum(\n",
    "        holiday_c_after - (g['Date'] - g['StateHolidayCLastDate']).dt.days\n",
    "        , 0)\n",
    "    g['Promo2Decay'] = g['Promo2Active'] * np.maximum(\n",
    "        promo2_after - (g['Date'] - g['Promo2StartedDate']).dt.days\n",
    "        , 0)\n",
    "    g['StateHolidayBNextDecay'] = (1 - g['StateHolidayB']) * np.maximum(\n",
    "        holiday_b_before - (g['StateHolidayBNextDate'] - g['Date']).dt.days\n",
    "        , 0)\n",
    "    g['StateHolidayCNextDecay'] = (1 - g['StateHolidayC']) * np.maximum(\n",
    "        holiday_c_before - (g['StateHolidayCNextDate'] - g['Date']).dt.days\n",
    "        , 0)\n",
    "\n",
    "\n",
    "make_decay_features(joined, promo_after=4, promo2_after=3,\n",
    "                    holiday_b_before=3, holiday_c_before=15, holiday_c_after=3)\n",
    "logger.debug(\"Decay features, new shape {shape}\".format(shape=joined.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_log_features(g, *features):\n",
    "    for f in features:\n",
    "        g[f] /= g[f].max()\n",
    "        g[\"{0}Log\".format(f)] = np.log1p(g[f])\n",
    "        g[\"{0}Log\".format(f)] /= g[\"{0}Log\".format(f)].max()\n",
    "\n",
    "\n",
    "scale_log_features(joined, *decay_features, 'DateTrend')\n",
    "logger.debug(\"Scale log features, new shape {shape}\".format(shape=joined.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_before_stairs(g, *features, days=(2, 3, 4, 5, 7, 14, 28)):\n",
    "    for f in features:\n",
    "        before = (g[f] - g['Date']).dt.days\n",
    "        for d in days:\n",
    "            g[\"{0}{1}before\".format(f, d)] = before.apply(\n",
    "                lambda s: 1 if 0 <= s < d else 0)\n",
    "\n",
    "\n",
    "make_before_stairs(joined, \"StateHolidayCNextDate\", \"StateHolidayBNextDate\",\n",
    "                   \"StateHolidayANextDate\", \"LongClosedNextDate\",\n",
    "                   days=stairs_steps)\n",
    "logger.debug(\"Before stairs features, new shape {shape}\".format(shape=joined.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_after_stairs(g, *features, days):\n",
    "    for f in features:\n",
    "        since = (g['Date'] - g[f]).dt.days\n",
    "        for d in days:\n",
    "            g[\"{0}{1}after\".format(f, d)] = since.apply(\n",
    "                lambda s: 1 if 0 <= s < d else 0)\n",
    "\n",
    "\n",
    "make_after_stairs(joined, \"PromoStartedLastDate\", days=(2, 3, 4))\n",
    "make_after_stairs(joined, \"StateHolidayCLastDate\", \"StateHolidayBLastDate\",\n",
    "                  \"Promo2StartedDate\", \"LongOpenLastDate\",\n",
    "                  days=stairs_steps)\n",
    "logger.debug(\"After stairs features, new shape {shape}\".format(shape=joined.shape))\n",
    "\n",
    "logger.debug(\"Splitting data into train and test, initial shape {shape}\".format(shape=joined.shape))\n",
    "train = joined[(train_range.min() <= joined['Date'])\n",
    "               & (joined['Date'] <= train_range.max())].drop('Id', axis=1)\n",
    "logger.debug(\"Train data shape {shape}\".format(shape=train.shape))\n",
    "\n",
    "test = joined[(test_range.min() <= joined['Date'])\n",
    "              & (joined['Date'] <= test_range.max())].drop(['Sales', 'Customers'], axis=1)\n",
    "logger.debug(\"Test data shape {shape}\".format(shape=test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trc = set(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tec = set(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tec.difference(trc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_stores = (\n",
    "  388, # most typical by svd of Sales time series\n",
    "  562, # second most typical by svd\n",
    "  851, # large gap in 2014\n",
    "  357  # small gap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_stores = ( 2,4,6,8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = train[train['Store'].apply(lambda s: s in example_stores)]\n",
    "# small_fold <- make_fold(small_train)\n",
    "\n",
    "# one_train <- filter(train, Store == 388)\n",
    "# one_test <- filter(test, Store == 388)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train['Date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "dataset = namedtuple('dataset', 'train, test, actual')\n",
    "\n",
    "\n",
    "def make_fold(train, step=1, predict_interval=6 * 7, step_by=7):\n",
    "    dates = pd.date_range(train['Date'].min(), train['Date'].max())\n",
    "    total = dates.shape[0]\n",
    "    last_train = total - predict_interval - (step - 1) * step_by\n",
    "    last_train_date = dates[last_train - 1]\n",
    "    last_predict = last_train + predict_interval\n",
    "    last_predict_date = dates[last_predict - 1]\n",
    "    train_set = train[train['Date'] <= last_train_date]\n",
    "    actual = train[(train['Date'] > last_train_date) & (train['Date'] <=\n",
    "                                                        last_predict_date)]\n",
    "    actual['Id'] = np.arange(1, actual.shape[0] + 1)\n",
    "    test_set = actual.drop('Sales', axis=1)\n",
    "    return dataset(train=train_set, test=test_set, actual=actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_fold = make_fold(small_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_fold[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_train = train[train['Store'] == 388]\n",
    "one_test = test[test['Store'] == 388]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[1]=A[0]+1\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.where((A[0]>=2) & (A[1]<=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.drop(A.index[(A[0]>=2) & (A[1]<=6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.columns = ['a','b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.query('a >= 2 & b <= 6 & c <= \"2010-04-02\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['c'] = pd.to_datetime('2010-04-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(4,2) > (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(list(test['Store'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.query('Store in {test_set_stores}'\n",
    "            .format(test_set_stores=list(test['Store'].unique())\n",
    "            ))['Store'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"a: {0}\".format(repr(test['Store'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(A['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"     aaa    \".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "    y = None\n",
    "    z = None\n",
    "    @classmethod\n",
    "    def get_data(cls, x):\n",
    "        cls.y = x - 1\n",
    "        cls.z = x + 1\n",
    "        \n",
    "    @classmethod\n",
    "    def show(cls):\n",
    "        print(\"y: {y}, z: {z}\".format(y=cls.y, z=cls.z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.get_data(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@allow_modifications(False)\n",
    "def expandA(A):\n",
    "    #     A = A.query('a > 2')\n",
    "    A['d']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expandA(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del A['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allow_modifications(allow):\n",
    "    def decorate(func):\n",
    "        func_name = func.__name__\n",
    "\n",
    "        def wrapper(_df, *args, **kwargs):\n",
    "            _df1 = _df.copy()\n",
    "            return_val = func(_df, *args, **kwargs)\n",
    "            try:\n",
    "                assert (allow and not _df1.equals(_df)) or (not allow and _df1.equals(_df))\n",
    "            except AssertionError:\n",
    "                if allow:\n",
    "                    logger.warning('!{f} does not modify dataframe!'.format(f=func_name))\n",
    "                else:\n",
    "                    logger.warning('!{f} is modifying dataframe!'.format(f=func_name))\n",
    "            else:\n",
    "                if not allow:\n",
    "                    logger.debug('{f} does not modify dataframe'.format(f=func_name))\n",
    "                else:\n",
    "                    logger.debug('{f} is modifying dataframe'.format(f=func_name))\n",
    "            return return_val\n",
    "        return wrapper\n",
    "\n",
    "    return decorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:Rossman]",
   "language": "python",
   "name": "conda-env-Rossman-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}